{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTSyP6CIU2hJIAmsUKiIXL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raz0208/ModernBERT/blob/main/ModernBERT_TokenEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import spacy"
      ],
      "metadata": {
        "id": "cZKSkc1hkz7d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load NLP model for noun phrase extraction\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load ModernBERT tokenizer and model from Hugging Face\n",
        "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "96X-_jMhkVud"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to gest text and return the embedding\n",
        "def get_text_embedding(text):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Forward pass to get hidden states\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the embeddings (use CLS token for sentence-level embedding)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [batch_size, hidden_size]\n",
        "\n",
        "    return cls_embedding.squeeze().numpy()"
      ],
      "metadata": {
        "id": "mbvLYg9klJuc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get per-token embeddings\n",
        "def get_token_embeddings(text):\n",
        "    # Tokenize with return_tensors and also get token strings\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, return_attention_mask=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)  # shape: [seq_len, hidden_size]\n",
        "\n",
        "    # Prepare cleaned token list and their embeddings\n",
        "    token_embeddings = []\n",
        "    for token, emb in zip(tokens, embeddings):\n",
        "        # Skip special tokens and punctuation/whitespace\n",
        "        if token in tokenizer.all_special_tokens:\n",
        "            continue\n",
        "        token = token.lstrip(\"Ġ▁\")  # Remove leading space markers from BPE/SentencePiece\n",
        "        if not token.isalnum():  # Skip non-alphanumeric tokens\n",
        "            continue\n",
        "        token_embeddings.append((token, emb.numpy()))\n",
        "\n",
        "    return token_embeddings"
      ],
      "metadata": {
        "id": "QLwE6v7q80Uh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract topic (noun phrase) from sentence\n",
        "def extract_main_topic(text):\n",
        "    doc = nlp(text)\n",
        "    noun_phrases = list(doc.noun_chunks)\n",
        "\n",
        "    # Heuristics: return the most informative-looking noun phrase\n",
        "    if noun_phrases:\n",
        "        # You could improve this with better heuristics or keyword detection\n",
        "        return max(noun_phrases, key=lambda chunk: len(chunk.text)).text.strip()\n",
        "    return None"
      ],
      "metadata": {
        "id": "DIxCEz-kL5e_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (Sentence: This is an application about Breast Cancer.)\n",
        "if __name__ == \"__main__\":\n",
        "    user_text = input(\"Enter your text: \")\n",
        "\n",
        "    # Get sentence embedding\n",
        "    embedding = get_text_embedding(user_text)\n",
        "    print(\"\\nSentence Embedding vector shape:\", embedding.shape)\n",
        "    print(\"Sentence Embedding (first 10 values):\", embedding[:10])\n",
        "\n",
        "    # Get per-token embeddings\n",
        "    token_embeddings = get_token_embeddings(user_text)\n",
        "    print(\"\\nToken-wise Embeddings:\")\n",
        "    for token, emb in token_embeddings:\n",
        "        print(f\"Token: {token:15} | Embedding (first 5 vals): {emb[:5]}\")\n",
        "\n",
        "    # Extract topic and embedding\n",
        "\n",
        "    ## Step 1: Extract topic\n",
        "    topic = extract_main_topic(user_text)\n",
        "    if topic:\n",
        "        print(f\"\\nIdentified topic: '{topic}'\")\n",
        "\n",
        "        ## Step 2: Get embedding for the topic\n",
        "        topic_embedding = get_text_embedding(topic)\n",
        "        print(\"Topic Embedding vector shape:\", topic_embedding.shape)\n",
        "        print(\"Topic Embedding (first 10 values):\", topic_embedding[:10])\n",
        "    else:\n",
        "        print(\"No topic found in the input.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weFFk2QDlHh0",
        "outputId": "e91c1ae7-42de-49b3-cbd1-d323188bf79e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your text: This is an application about Breast Cancer.\n",
            "\n",
            "Sentence Embedding vector shape: (768,)\n",
            "Sentence Embedding (first 10 values): [ 0.42236355 -0.8862073  -0.6536694  -0.2981413  -0.5874422  -0.720903\n",
            " -0.8588484  -0.89695704  0.5856571  -0.9214181 ]\n",
            "\n",
            "Token-wise Embeddings:\n",
            "Token: This            | Embedding (first 5 vals): [-0.6776887  -0.92234474 -0.38191086  0.1988687   0.6651754 ]\n",
            "Token: is              | Embedding (first 5 vals): [ 0.55707484 -1.9195614  -0.5415806   0.16398725  0.47974288]\n",
            "Token: an              | Embedding (first 5 vals): [ 0.8371718  -1.4686286  -0.6741889   0.5709336  -0.00925406]\n",
            "Token: application     | Embedding (first 5 vals): [ 0.34761772 -2.8377428   0.42626727 -0.28745332  1.098285  ]\n",
            "Token: about           | Embedding (first 5 vals): [ 1.9028491  -1.1237415  -0.74431485 -0.17381278 -0.2998082 ]\n",
            "Token: Breast          | Embedding (first 5 vals): [ 1.344319   -1.9794563  -1.4910724   0.20805806  0.18633802]\n",
            "Token: Cancer          | Embedding (first 5 vals): [ 1.0337043  -0.93953913 -0.7409618  -1.0466464   1.0365046 ]\n",
            "\n",
            "Identified topic: 'an application'\n",
            "Topic Embedding vector shape: (768,)\n",
            "Topic Embedding (first 10 values): [ 0.47918996  0.05242749 -0.03438827 -0.10434122 -0.06480165  0.31475833\n",
            "  0.0550458  -0.280453    0.17244111 -0.39741275]\n"
          ]
        }
      ]
    }
  ]
}