{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqj39dL/F/nGW7nC3QYiY+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raz0208/ModernBERT/blob/main/ModernBERT_TokenEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract embedding form inpot text using ModernBERT"
      ],
      "metadata": {
        "id": "yUXTO1ky7om9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "import re"
      ],
      "metadata": {
        "id": "cZKSkc1hkz7d"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load NLP and ModernBert models"
      ],
      "metadata": {
        "id": "sW3Mnnjp71Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load NLP model for noun phrase extraction\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load ModernBERT tokenizer and model from Hugging Face\n",
        "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "96X-_jMhkVud"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus (text) cleaning"
      ],
      "metadata": {
        "id": "BfSJZDwd77Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text\n",
        "def clean_text_for_topics(text):\n",
        "    # Remove content in parentheses (e.g., (2015), (note))\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "\n",
        "    # Remove percentages, currency, units like \"mg\", \"billion\", \"years\", etc.\n",
        "    text = re.sub(r'\\b\\d+(\\.\\d+)?\\s*(%|percent|million|billion|thousand|mg|kg|g|km|cm|years?|months?|days?)\\b', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove standalone numbers (integers, decimals, years)\n",
        "    text = re.sub(r'\\b\\d+(\\.\\d+)?\\b', '', text)\n",
        "\n",
        "    # Remove numeric ranges (e.g., 2020–2023 or 10-15)\n",
        "    text = re.sub(r'\\b\\d{2,4}\\s*[-–—]\\s*\\d{2,4}\\b', '', text)\n",
        "\n",
        "    # Remove metadata indicators like \"Deaths:\", \"Rate:\", \"Total:\"\n",
        "    text = re.sub(r'\\b(Deaths?|Rate|Cases?|Total|Incidence|Prevalence|Statistics):?.*', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove slashes and colons separating stats or metadata\n",
        "    text = re.sub(r'[:/]', ' ', text)\n",
        "\n",
        "    # Remove all punctuation except commas\n",
        "    text = re.sub(r'[^\\w\\s,]', '', text)\n",
        "\n",
        "    # Collapse extra spaces\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "nLpMNhV9z042"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract emmbedings based on full text and each token separatly."
      ],
      "metadata": {
        "id": "9YZitYhr8Bmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to gest text and return the embedding\n",
        "def get_text_embedding(text):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Forward pass to get hidden states\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the embeddings (use CLS token for sentence-level embedding)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [batch_size, hidden_size]\n",
        "\n",
        "    return cls_embedding.squeeze().numpy()"
      ],
      "metadata": {
        "id": "mbvLYg9klJuc"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get per-token embeddings\n",
        "def get_token_embeddings(text):\n",
        "\n",
        "    # Call text cleaning function\n",
        "    text = clean_text_for_topics(text)\n",
        "\n",
        "    # Tokenize with return_tensors and also get token strings\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, return_attention_mask=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)  # shape: [seq_len, hidden_size]\n",
        "\n",
        "    # Prepare cleaned token list and their embeddings\n",
        "    token_embeddings = []\n",
        "    for token, emb in zip(tokens, embeddings):\n",
        "        # Skip special tokens and punctuation/whitespace\n",
        "        if token in tokenizer.all_special_tokens:\n",
        "            continue\n",
        "        token = token.lstrip(\"Ġ▁\")  # Remove leading space markers from BPE/SentencePiece\n",
        "        if not token.isalnum():  # Skip non-alphanumeric tokens\n",
        "            continue\n",
        "        token_embeddings.append((token, emb.numpy()))\n",
        "\n",
        "    return token_embeddings"
      ],
      "metadata": {
        "id": "QLwE6v7q80Uh"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Topic extraction: First approch"
      ],
      "metadata": {
        "id": "KeiVd5Re8Ps5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### --- ### Basic method for topic sxtraction ### --- ###\n",
        "\n",
        "# # Extract topics (noun phrase) from sentence\n",
        "# def extract_topics(text):\n",
        "#     doc = nlp(text)\n",
        "#     topics = []\n",
        "\n",
        "#     for chunk in doc.noun_chunks:\n",
        "#         # Remove articles/determiners (like \"the\", \"an\", \"a\")\n",
        "#         words = [token.text for token in chunk if token.pos_ != \"DET\"]\n",
        "#         if words:\n",
        "#             cleaned_phrase = \" \".join(words).strip()\n",
        "#             topics.append(cleaned_phrase)\n",
        "\n",
        "#     # Remove duplicates while preserving order\n",
        "#     seen = set()\n",
        "#     unique_topics = []\n",
        "#     for topic in topics:\n",
        "#         if topic.lower() not in seen:\n",
        "#             unique_topics.append(topic)\n",
        "#             seen.add(topic.lower())\n",
        "\n",
        "#     return unique_topics"
      ],
      "metadata": {
        "id": "DIxCEz-kL5e_"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### --- ### Second method for topic sxtraction ### --- ###\n",
        "\n",
        "# # Extract topics (noun phrase) from sentence\n",
        "# def extract_topics(text):\n",
        "#     doc = nlp(text)\n",
        "#     topics = []\n",
        "\n",
        "#     # 1. Add noun chunks (excluding determiners)\n",
        "#     for chunk in doc.noun_chunks:\n",
        "#         words = [token.text for token in chunk if token.pos_ != \"DET\"]\n",
        "#         if words:\n",
        "#             cleaned = \" \".join(words).strip()\n",
        "#             topics.append(cleaned)\n",
        "\n",
        "#     # 2. Add standalone noun tokens (important for medical terms like \"diabetes\")\n",
        "#     for token in doc:\n",
        "#         if token.pos_ == \"NOUN\" and not token.is_stop:\n",
        "#             topics.append(token.text.strip())\n",
        "\n",
        "#     # 3. Add named entities related to medical types\n",
        "#     for ent in doc.ents:\n",
        "#         if ent.label_ in {\"DISEASE\", \"CONDITION\", \"MEDICAL_CONDITION\", \"HEALTH_CONDITION\", \"SYMPTOM\"}:\n",
        "#             topics.append(ent.text.strip())\n",
        "\n",
        "#     # 4. Clean and deduplicate\n",
        "#     seen = set()\n",
        "#     unique_topics = []\n",
        "#     for topic in topics:\n",
        "#         topic_clean = topic.lower()\n",
        "#         if topic_clean not in seen and topic_clean.isalpha():\n",
        "#             unique_topics.append(topic)\n",
        "#             seen.add(topic_clean)\n",
        "\n",
        "#     return unique_topics"
      ],
      "metadata": {
        "id": "FejKehEIiwkj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### --- ### Third method for topic sxtraction ### --- ###\n",
        "\n",
        "# Extract topics (noun phrase) from sentence\n",
        "def extract_topics(text):\n",
        "    doc = nlp(text)\n",
        "    topics = []\n",
        "\n",
        "    for chunk in doc.noun_chunks:\n",
        "        # Keep only noun-based chunks\n",
        "        if all(token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"] for token in chunk if not token.is_stop):\n",
        "            topic = \" \".join(token.text for token in chunk if token.pos_ != \"DET\").strip()\n",
        "            topics.append(topic)\n",
        "\n",
        "    # Fallback: add standalone nouns that weren't in noun_chunks\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\", \"PROPN\"] and not token.is_stop:\n",
        "            if not any(token.text in topic for topic in topics):\n",
        "                topics.append(token.text)\n",
        "\n",
        "    # Remove duplicates\n",
        "    seen = set()\n",
        "    unique_topics = []\n",
        "    for topic in topics:\n",
        "        if topic.lower() not in seen:\n",
        "            unique_topics.append(topic)\n",
        "            seen.add(topic.lower())\n",
        "\n",
        "    return unique_topics"
      ],
      "metadata": {
        "id": "gz22zUHpwnmO"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the code"
      ],
      "metadata": {
        "id": "CfAA2PiN8Twh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TopicSecond approch\n",
        "- Extract the representative topic based on the input text."
      ],
      "metadata": {
        "id": "AX_aXtil_9AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "lvRzxrNfAKmw"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract representative topic\n",
        "def extract_main_topic(text):\n",
        "    \"\"\"\n",
        "    Extract the most representative topic from the input text using ModernBERT embeddings.\n",
        "    \"\"\"\n",
        "    # Step 1: Extract all topics from text\n",
        "    candidate_topics = extract_topics(text)\n",
        "\n",
        "    if not candidate_topics:\n",
        "        return None, None\n",
        "\n",
        "    # Step 2: Get embedding for full input text\n",
        "    text_embedding = get_text_embedding(text).reshape(1, -1)\n",
        "\n",
        "    # Step 3: Compute embeddings and similarities for each candidate topic\n",
        "    best_topic = None\n",
        "    highest_similarity = -1\n",
        "\n",
        "    for topic in candidate_topics:\n",
        "        topic_embedding = get_text_embedding(topic).reshape(1, -1)\n",
        "        similarity = cosine_similarity(text_embedding, topic_embedding)[0][0]\n",
        "\n",
        "        if similarity > highest_similarity:\n",
        "            highest_similarity = similarity\n",
        "            best_topic = topic\n",
        "\n",
        "    return best_topic, highest_similarity"
      ],
      "metadata": {
        "id": "JpZetaVFAGd3"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### --- ### Sample text for test ### --- ###\n",
        "\n",
        "# 1- This is an application about Breast Cancer.\n",
        "# 2- Treating high blood pressure, high blood lipids, diabetes\n",
        "# 3- Heart failure, heart attack, stroke, aneurysm, peripheral artery disease, sudden cardiac arrest. Deaths: 17.9 million / 32% (2015)"
      ],
      "metadata": {
        "id": "QNYU-UXcxlQV"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (Sentence: This is an application about Breast Cancer.)\n",
        "if __name__ == \"__main__\":\n",
        "    user_text = input(\"Enter your text: \")\n",
        "\n",
        "    # Get sentence embedding\n",
        "    embedding = get_text_embedding(user_text)\n",
        "    print(\"\\nSentence Embedding vector shape:\", embedding.shape)\n",
        "    print(\"Sentence Embedding (first 10 values):\", embedding[:10])\n",
        "\n",
        "    print(\"\\n\", \"####\"*10)\n",
        "\n",
        "    # Get per-token embeddings\n",
        "    token_embeddings = get_token_embeddings(user_text)\n",
        "    print(\"\\nToken-wise Embeddings:\")\n",
        "    for token, emb in token_embeddings:\n",
        "        print(f\"Token: {token:15} | Embedding (first 5 vals): {emb[:5]}\")\n",
        "\n",
        "    print(\"\\n\", \"####\"*10)\n",
        "\n",
        "    # Extract topic and embedding\n",
        "    topics = extract_topics(user_text)\n",
        "    if topics:\n",
        "        print(f\"\\nIdentified topics: {topics}\")\n",
        "        for topic in topics:\n",
        "            topic_embedding = get_text_embedding(topic)\n",
        "            print(f\"\\nTopic: '{topic}'\")\n",
        "            print(\"Embedding shape:\", topic_embedding.shape)\n",
        "            print(\"Embedding (first 5 values):\", topic_embedding[:5])\n",
        "    else:\n",
        "        print(\"No topics found.\")\n",
        "\n",
        "    print(\"\\n\", \"####\"*10)\n",
        "\n",
        "    # Extract main topic\n",
        "    Representative_topic, similarity = extract_main_topic(user_text)\n",
        "    if Representative_topic:\n",
        "        print(f\"\\nRepresentative Topic: '{Representative_topic}' (similarity: {similarity:.4f})\")\n",
        "\n",
        "        # Get sentence embedding\n",
        "        RepTopic_embedding = get_text_embedding(Representative_topic)\n",
        "        print(\"\\nSentence Embedding vector shape:\", RepTopic_embedding.shape)\n",
        "        print(\"Sentence Embedding (first 10 values):\", RepTopic_embedding[:10])\n",
        "    else:\n",
        "        print(\"No main topic found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weFFk2QDlHh0",
        "outputId": "c966b611-b6a2-4f69-a9f9-8ee8e5270417"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your text: Heart failure, heart attack, stroke, aneurysm, peripheral artery disease, sudden cardiac arrest. Deaths: 17.9 million / 32% (2015)\n",
            "\n",
            "Sentence Embedding vector shape: (768,)\n",
            "Sentence Embedding (first 10 values): [ 0.43869126 -0.26175603 -0.7007977   0.22565924 -0.38932806 -0.41834965\n",
            " -1.1746391  -0.8032964   0.19345134 -0.10638831]\n",
            "\n",
            " ########################################\n",
            "\n",
            "Token-wise Embeddings:\n",
            "Token: Heart           | Embedding (first 5 vals): [ 2.1719382  -0.5325865  -0.69776136  0.8575297   0.06485313]\n",
            "Token: failure         | Embedding (first 5 vals): [ 2.1571448  -2.0265424  -0.13466616 -0.95061946  0.09105098]\n",
            "Token: heart           | Embedding (first 5 vals): [ 1.0728792  -0.02802396 -0.36255845  1.1986513   1.0817125 ]\n",
            "Token: attack          | Embedding (first 5 vals): [ 2.1625645  -0.5557532  -0.5867817   0.46016052  1.0440698 ]\n",
            "Token: stroke          | Embedding (first 5 vals): [ 1.8419516  -0.65602314 -0.7234627   0.06608873  1.2474613 ]\n",
            "Token: aneurysm        | Embedding (first 5 vals): [ 0.15772632 -1.7048483  -0.85961723 -0.1381113   0.3875144 ]\n",
            "Token: peripheral      | Embedding (first 5 vals): [-1.1645821  -0.41944495 -1.7582278  -1.0087764  -0.71015644]\n",
            "Token: artery          | Embedding (first 5 vals): [ 0.9796437  -0.9116391  -0.7135119  -0.36534902  0.8482507 ]\n",
            "Token: disease         | Embedding (first 5 vals): [ 0.6137189  -0.9104982  -0.26123503 -0.15676133  0.53263396]\n",
            "Token: sudden          | Embedding (first 5 vals): [-0.6643993  -0.19043088 -1.0873909  -0.38163525 -0.60644174]\n",
            "Token: cardiac         | Embedding (first 5 vals): [ 1.503649  -1.0538021 -1.1808113  0.5497633  1.5003076]\n",
            "Token: arrest          | Embedding (first 5 vals): [ 1.2412406  -2.149337   -0.15059394 -0.03853888  0.70850176]\n",
            "\n",
            " ########################################\n",
            "\n",
            "Identified topics: ['Heart failure', 'heart attack', 'stroke', 'aneurysm', 'peripheral artery disease', 'sudden cardiac arrest', 'Deaths', '%']\n",
            "\n",
            "Topic: 'Heart failure'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.16492663  0.02263031  0.21004711  0.06224616 -0.02963544]\n",
            "\n",
            "Topic: 'heart attack'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.94546443 -1.028872   -0.8995476  -0.31431967 -0.8673811 ]\n",
            "\n",
            "Topic: 'stroke'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.30824003  1.1333296  -0.49130827  0.3495477  -0.87221336]\n",
            "\n",
            "Topic: 'aneurysm'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.10142252 -0.0001621  -0.12641908 -0.02018686 -0.04443587]\n",
            "\n",
            "Topic: 'peripheral artery disease'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.13049817  0.0110046   0.21238305  0.04699522 -0.03031034]\n",
            "\n",
            "Topic: 'sudden cardiac arrest'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.14497575  0.05025197  0.12568915  0.02223136 -0.03334851]\n",
            "\n",
            "Topic: 'Deaths'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.14946249  0.01291869  0.21200925  0.0697582  -0.06043363]\n",
            "\n",
            "Topic: '%'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.17959426  0.11120617  0.10759397  0.07044373 -0.02322442]\n",
            "\n",
            " ########################################\n",
            "\n",
            "Representative Topic: 'heart attack' (similarity: 0.8783)\n",
            "\n",
            "Sentence Embedding vector shape: (768,)\n",
            "Sentence Embedding (first 10 values): [ 0.94546443 -1.028872   -0.8995476  -0.31431967 -0.8673811  -1.0555787\n",
            " -0.13792875 -0.7882952   0.68352544  0.08373442]\n"
          ]
        }
      ]
    }
  ]
}