{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSfoNv9J23reKZBS5m1teT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raz0208/ModernBERT/blob/main/ModernBERT_TokenEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import spacy\n",
        "from keybert import KeyBERT"
      ],
      "metadata": {
        "id": "cZKSkc1hkz7d"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load NLP model for noun phrase extraction\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load ModernBERT tokenizer and model from Hugging Face\n",
        "MODEL_NAME = \"answerdotai/ModernBERT-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "96X-_jMhkVud"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to gest text and return the embedding\n",
        "def get_text_embedding(text):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Forward pass to get hidden states\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the embeddings (use CLS token for sentence-level embedding)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: [batch_size, hidden_size]\n",
        "\n",
        "    return cls_embedding.squeeze().numpy()"
      ],
      "metadata": {
        "id": "mbvLYg9klJuc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get per-token embeddings\n",
        "def get_token_embeddings(text):\n",
        "    # Tokenize with return_tensors and also get token strings\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, return_attention_mask=True)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    embeddings = outputs.last_hidden_state.squeeze(0)  # shape: [seq_len, hidden_size]\n",
        "\n",
        "    # Prepare cleaned token list and their embeddings\n",
        "    token_embeddings = []\n",
        "    for token, emb in zip(tokens, embeddings):\n",
        "        # Skip special tokens and punctuation/whitespace\n",
        "        if token in tokenizer.all_special_tokens:\n",
        "            continue\n",
        "        token = token.lstrip(\"Ġ▁\")  # Remove leading space markers from BPE/SentencePiece\n",
        "        if not token.isalnum():  # Skip non-alphanumeric tokens\n",
        "            continue\n",
        "        token_embeddings.append((token, emb.numpy()))\n",
        "\n",
        "    return token_embeddings"
      ],
      "metadata": {
        "id": "QLwE6v7q80Uh"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### --- ### Basic method for topic sxtraction ### --- ###\n",
        "\n",
        "# # Extract topics (noun phrase) from sentence\n",
        "# def extract_topics(text):\n",
        "#     doc = nlp(text)\n",
        "#     topics = []\n",
        "\n",
        "#     for chunk in doc.noun_chunks:\n",
        "#         # Remove articles/determiners (like \"the\", \"an\", \"a\")\n",
        "#         words = [token.text for token in chunk if token.pos_ != \"DET\"]\n",
        "#         if words:\n",
        "#             cleaned_phrase = \" \".join(words).strip()\n",
        "#             topics.append(cleaned_phrase)\n",
        "\n",
        "#     # Remove duplicates while preserving order\n",
        "#     seen = set()\n",
        "#     unique_topics = []\n",
        "#     for topic in topics:\n",
        "#         if topic.lower() not in seen:\n",
        "#             unique_topics.append(topic)\n",
        "#             seen.add(topic.lower())\n",
        "\n",
        "#     return unique_topics"
      ],
      "metadata": {
        "id": "DIxCEz-kL5e_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### --- ### Second method for topic sxtraction ### --- ###\n",
        "\n",
        "# Extract topics (noun phrase) from sentence\n",
        "def extract_topics(text):\n",
        "    doc = nlp(text)\n",
        "    topics = []\n",
        "\n",
        "    # 1. Add noun chunks (excluding determiners)\n",
        "    for chunk in doc.noun_chunks:\n",
        "        words = [token.text for token in chunk if token.pos_ != \"DET\"]\n",
        "        if words:\n",
        "            cleaned = \" \".join(words).strip()\n",
        "            topics.append(cleaned)\n",
        "\n",
        "    # 2. Add standalone noun tokens (important for medical terms like \"diabetes\")\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"NOUN\" and not token.is_stop:\n",
        "            topics.append(token.text.strip())\n",
        "\n",
        "    # 3. Add named entities related to medical types\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in {\"DISEASE\", \"CONDITION\", \"MEDICAL_CONDITION\", \"HEALTH_CONDITION\", \"SYMPTOM\"}:\n",
        "            topics.append(ent.text.strip())\n",
        "\n",
        "    # 4. Clean and deduplicate\n",
        "    seen = set()\n",
        "    unique_topics = []\n",
        "    for topic in topics:\n",
        "        topic_clean = topic.lower()\n",
        "        if topic_clean not in seen and topic_clean.isalpha():\n",
        "            unique_topics.append(topic)\n",
        "            seen.add(topic_clean)\n",
        "\n",
        "    return unique_topics"
      ],
      "metadata": {
        "id": "FejKehEIiwkj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (Sentence: This is an application about Breast Cancer.)\n",
        "if __name__ == \"__main__\":\n",
        "    user_text = input(\"Enter your text: \")\n",
        "\n",
        "    # Get sentence embedding\n",
        "    embedding = get_text_embedding(user_text)\n",
        "    print(\"\\nSentence Embedding vector shape:\", embedding.shape)\n",
        "    print(\"Sentence Embedding (first 10 values):\", embedding[:10])\n",
        "\n",
        "    # Get per-token embeddings\n",
        "    token_embeddings = get_token_embeddings(user_text)\n",
        "    print(\"\\nToken-wise Embeddings:\")\n",
        "    for token, emb in token_embeddings:\n",
        "        print(f\"Token: {token:15} | Embedding (first 5 vals): {emb[:5]}\")\n",
        "\n",
        "    # Extract topic and embedding\n",
        "\n",
        "    topics = extract_topics(user_text)\n",
        "    if topics:\n",
        "        print(f\"\\nIdentified topics: {topics}\")\n",
        "        for topic in topics:\n",
        "            topic_embedding = get_text_embedding(topic)\n",
        "            print(f\"\\nTopic: '{topic}'\")\n",
        "            print(\"Embedding shape:\", topic_embedding.shape)\n",
        "            print(\"Embedding (first 5 values):\", topic_embedding[:5])\n",
        "    else:\n",
        "        print(\"No topics found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weFFk2QDlHh0",
        "outputId": "3ffa1eaf-cf8d-4934-b487-8cdacf27a5e8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your text: Treating high blood pressure, high blood lipids, diabetes.\n",
            "\n",
            "Sentence Embedding vector shape: (768,)\n",
            "Sentence Embedding (first 10 values): [ 0.77355295 -0.9813839  -0.6875638  -0.36766133 -0.45456663 -1.140104\n",
            " -1.0688002  -0.98096883  0.03701263  0.48558575]\n",
            "\n",
            "Token-wise Embeddings:\n",
            "Token: Treat           | Embedding (first 5 vals): [ 0.9773896   1.1849611  -0.41335937 -0.42269483  0.30082476]\n",
            "Token: ing             | Embedding (first 5 vals): [ 0.5608321  -1.0576426   0.23242453 -0.13428017 -0.4607672 ]\n",
            "Token: high            | Embedding (first 5 vals): [ 2.578935   -1.1134218   0.05234717 -0.05968764 -0.13070378]\n",
            "Token: blood           | Embedding (first 5 vals): [ 1.5280033   0.06228757 -0.822701    0.5588437   1.3647466 ]\n",
            "Token: pressure        | Embedding (first 5 vals): [ 1.1171628 -1.0177413  0.0522917 -0.3610779  1.4506999]\n",
            "Token: high            | Embedding (first 5 vals): [ 0.9468698  -0.3100607   0.46468502  1.1450183   0.40300363]\n",
            "Token: blood           | Embedding (first 5 vals): [ 1.441571    0.23721448 -1.1504577   0.7351728   1.0211792 ]\n",
            "Token: lipids          | Embedding (first 5 vals): [ 1.6617854 -0.6666119 -0.9485702 -1.5970436  1.060559 ]\n",
            "Token: diabetes        | Embedding (first 5 vals): [ 0.32333815  0.8051268  -1.4277933   1.0127367   1.4846798 ]\n",
            "\n",
            "Identified topics: ['blood', 'pressure', 'lipids', 'diabetes']\n",
            "\n",
            "Topic: 'blood'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 1.6797935   0.6406126  -1.0055819  -0.02133311  0.07616102]\n",
            "\n",
            "Topic: 'pressure'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.18564391 -0.02010763 -0.01667174 -0.08303881 -0.04276595]\n",
            "\n",
            "Topic: 'lipids'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.12886737  0.0270218   0.20761591  0.09819701 -0.043161  ]\n",
            "\n",
            "Topic: 'diabetes'\n",
            "Embedding shape: (768,)\n",
            "Embedding (first 5 values): [ 0.14053792  0.02462899  0.22877301  0.03016384 -0.03586877]\n"
          ]
        }
      ]
    }
  ]
}